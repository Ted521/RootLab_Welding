{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6070be-8163-400d-a09e-6a63a4df68da",
   "metadata": {},
   "source": [
    "<img src='ipydata/용접.png' align='center' width=\"1500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b3e1d-1d47-4152-98f7-3ff505823b23",
   "metadata": {},
   "source": [
    "<ul><li style=\"font-size:2em;\">제조 데이터 준비</li></ul>\n",
    "<img src=\"ipydata/16.png\" style=\"display:block; margin:0 auto; width:700px;\">\n",
    "<br>\n",
    "<span style=\"font-size:20px;\">사용대상: 용접 뿌리업종 내 용접물에서 발생하는 이미지 제조데이터(Vision Machine활용)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05543f97-6b36-4423-a370-997ac9f513ab",
   "metadata": {},
   "source": [
    "<ul><li style=\"font-size:2em;\">제조물</li></ul>\n",
    "<div style=\"width:1200px; margin:0 auto;\">\n",
    "<a href=\"#\"><img style=\"width:400px; height:250px\" src=\"ipydata/67.png\"></a>\n",
    "<a href=\"#\"><img style=\"width:400px; height:250px\" src=\"ipydata/71.png\"></a>\n",
    "</div>\n",
    "<br>\n",
    "<span style=\"font-size:20px;\">선박 제작용 부속 용접 접착물</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e20f5-a833-4015-b574-5420849406e6",
   "metadata": {},
   "source": [
    "<ul><li style=\"font-size:2em;\">현장문제</li></ul>\n",
    "<img src=\"ipydata/육안.png\" style=\"display:block; margin:0 auto; width:600px\">\n",
    "<br>\n",
    "<span style=\"font-size:20px;\">공정시 산출물로 용접 접착물에서 용접이 제대로 이루어졌는지에 대한 검사가 육안 및 노하우를 통해 진행되고 있음</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7f388-279f-473e-a9f5-b079c5c5d6be",
   "metadata": {},
   "source": [
    "<ul><li style=\"font-size:2em;\">제조AI 개발 목적</li></ul>\n",
    "<img src=\"ipydata/판정.png\" style=\"display:block; margin:0 auto; width:600px\">\n",
    "<br>\n",
    "<span style=\"font-size:20px;\">비전 시스템을 활용하여 용접 비드면을 확인하고, 용접 비드면에서 발생하는 특징점을 찾아내어 용접 상태가 양/불인지 확인하기 위함</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be655fa-64f3-444d-9268-58eb79306db3",
   "metadata": {},
   "source": [
    "# 1. 용접 솔루션 학습 및 테스트\n",
    "용접솔루션 학습 및 테스트 방법을 간단한 소스와 함께 기술합니다.\n",
    "\n",
    "## 용접솔루션 제조데이터셋 디렉토리 구조\n",
    " 명장에서 다운로드 받은 용접솔루션 제조데이터넷을 data 하위에 업로드한다.<br /> \n",
    " 용접 솔루션은 100건의 제조데이터셋을 80:20으로 용접 솔루션은 3001건의 제조데이터셋을 무작위 분류하여<br />\n",
    " 학습용 제조데이터는 train.txt에 테스트용 제조데이터는 val.txt에 등록한다.\n",
    " \n",
    " 제조데이터셋 디렉토리 구조는 다음과 같다.\n",
    "  \n",
    " ~~~\n",
    "    data/\n",
    "        WeldingDataset/ \n",
    "            images/\n",
    "            labels/\n",
    "            splits/\n",
    "                train.txt # 학습용 제조데이터 파일명 목록\n",
    "                val.txt   # 테스트용 제조데이터 파일명 목록\n",
    "~~~                \n",
    "\n",
    "## 용접 솔루션 이미지 비드면 탐지 제조데이터셋 예제\n",
    "---\n",
    "<div style=\"display:flex\">\n",
    "    <div style=\"flex:1;padding-right:10px;\">\n",
    "        <img src=\"data/WeldingDataset/images/22.jpg\" width=\"400px\"/>\n",
    "        <br /><span style=\"display:inline-block;width:400px;text-align:center;\"><strong>원본이미지(images/22.jpg)</strong></span>\n",
    "    </div>\n",
    "    <div style=\"flex:1;padding-left:10px;\">\n",
    "        <img src=\"data/WeldingDataset/labels/22.png\" width=\"400px\"/>\n",
    "        <br /><span style=\"display:inline-block;width:400px;text-align:center;\"><strong>어노테이션이미지(labels/22.png)</strong></span>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465a955",
   "metadata": {},
   "source": [
    "## 2. 학습 및 테스트 소스코드 작성\n",
    "\n",
    "\n",
    "### 모듈 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3a2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# from tqdm import tqdm\n",
    "import mmcv\n",
    "import torch\n",
    "from dotmap import DotMap\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmcv.runner import build_optimizer, build_runner, load_checkpoint\n",
    "from mmseg.datasets.custom import Compose\n",
    "from mmseg.models.builder import build_segmentor\n",
    "\n",
    "# util.py 파일 참조\n",
    "from util import Configuration, build_dataset, build_dataloader, LoadImage, show_test_result_pyplot, get_root_logger, \\\n",
    "    DistEvalHook, EvalHook, show_result_pyplot, intersect_and_union, divide_img, aggregate_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a98155",
   "metadata": {},
   "source": [
    "### 인자 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7b8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_arguments():\n",
    "    args = DotMap()\n",
    "\n",
    "    args.max_iters = 2000  # max iteration 최대의 성능은 100,000번 학습 권장 \n",
    "    args.log_interval = 500  # 20 iteration에 한 번씩 로깅\n",
    "    args.evaluation_interval = 500  # 200 iteration에 한 번씩 evaluation\n",
    "    args.checkpoint_interval = 500  # 200 iteration에 한 번씩 checkpoint 저장\n",
    "    args.batch_size = 2 # tested 8\n",
    "    args.crop_size = [512, 512]  # 이미지 분할 크기 지정\n",
    "    args.num_worker = 2\n",
    "    args.seed = 0\n",
    "    # 학습 및 테스트 모델 환경설정 파일 경로\n",
    "    # args.cfg_file = 'configs/pspnet_r50-d8_512x1024_40k_cityscapes.py'\n",
    "    args.cfg_file = 'configs/configs.py'\n",
    "\n",
    "    # 학습 또는 테스트에 사용할 용접 이미지 제조데이터의 최상위 경로로 명장플랫폼에서 다운로드 받은 제조데이터셋을 다운로드 받아\n",
    "    # data 에 압축을 해제한다.\n",
    "    # WeldingDataset 하위의 images, labels, splits 디렉토리에 파일을 확인하고\n",
    "    # splits 하위에 학습용 이미지 이름 목록을 train.txt, 테스트용 이미지 이름 목록을 val.txt 에 저장한다. 현재는 학습:테스트를 8:2 으로 랜덤하게\n",
    "    # 배분하였다.\n",
    "    data = 'data'\n",
    "\n",
    "    # 학습 결과 checkpoint 파일 저장 및 테스트 결과 이미지 저장 경로\n",
    "    args.work_dir = os.path.join(data, 'work')\n",
    "    args.test_dir = 'test_image'\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0d62c",
   "metadata": {},
   "source": [
    "### 용접 비드면 탐지 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bad12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeldingSegmentor:\n",
    "\n",
    "    def __init__(self, args, device):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:  전달인자\n",
    "        :param device: 사용할 장치\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "\n",
    "    def train_segmentor(self, model,\n",
    "                        dataset,\n",
    "                        cfg,\n",
    "                        distributed=False,\n",
    "                        validate=False,\n",
    "                        timestamp=None,\n",
    "                        meta=None):\n",
    "        \"\"\"Launch segmentor training.\"\"\"\n",
    "        logger = get_root_logger(cfg.log_level)\n",
    "\n",
    "        # prepare data loaders\n",
    "        dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset]\n",
    "        data_loaders = [\n",
    "            build_dataloader(\n",
    "                ds,\n",
    "                cfg.data.samples_per_gpu,\n",
    "                cfg.data.workers_per_gpu,\n",
    "                # cfg.gpus will be ignored if distributed\n",
    "                len(cfg.gpu_ids),\n",
    "                dist=distributed,\n",
    "                seed=cfg.seed,\n",
    "                drop_last=True) for ds in dataset\n",
    "        ]\n",
    "\n",
    "        # put model on gpus\n",
    "        if distributed:\n",
    "            find_unused_parameters = cfg.get('find_unused_parameters', False)\n",
    "            # Sets the `find_unused_parameters` parameter in\n",
    "            # torch.nn.parallel.DistributedDataParallel\n",
    "            model = MMDistributedDataParallel(\n",
    "                model.cuda(),\n",
    "                device_ids=[torch.cuda.current_device()],\n",
    "                broadcast_buffers=False,\n",
    "                find_unused_parameters=find_unused_parameters)\n",
    "        else:\n",
    "            model = MMDataParallel(\n",
    "                model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)\n",
    "\n",
    "        # build runner\n",
    "        optimizer = build_optimizer(model, cfg.optimizer)\n",
    "\n",
    "        if cfg.get('runner') is None:\n",
    "            cfg.runner = {'type': 'IterBasedRunner', 'max_iters': cfg.total_iters}\n",
    "            warnings.warn(\n",
    "                'config is now expected to have a `runner` section, '\n",
    "                'please set `runner` in your config.', UserWarning)\n",
    "\n",
    "        runner = build_runner(\n",
    "            cfg.runner,\n",
    "            default_args=dict(\n",
    "                model=model,\n",
    "                batch_processor=None,\n",
    "                optimizer=optimizer,\n",
    "                work_dir=cfg.work_dir,\n",
    "                logger=logger,\n",
    "                meta=meta))\n",
    "\n",
    "        # register hooks\n",
    "        runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n",
    "                                       cfg.checkpoint_config, cfg.log_config,\n",
    "                                       cfg.get('momentum_config', None))\n",
    "\n",
    "        # an ugly walkaround to make the .log and .log.json filenames the same\n",
    "        runner.timestamp = timestamp\n",
    "\n",
    "        # register eval hooks\n",
    "        if validate:\n",
    "            print(cfg.data.val)\n",
    "            val_dataset = build_dataset(cfg.data.val, dict(test_mode=True))\n",
    "            val_dataloader = build_dataloader(\n",
    "                val_dataset,\n",
    "                samples_per_gpu=1,\n",
    "                workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "                dist=distributed,\n",
    "                shuffle=False)\n",
    "            eval_cfg = cfg.get('evaluation', {})\n",
    "            eval_cfg['by_epoch'] = cfg.runner['type'] != 'IterBasedRunner'\n",
    "            eval_hook = DistEvalHook if distributed else EvalHook\n",
    "            runner.register_hook(eval_hook(val_dataloader, **eval_cfg), priority='LOW')\n",
    "\n",
    "        if cfg.resume_from:\n",
    "            runner.resume(cfg.resume_from)\n",
    "        elif cfg.load_from:\n",
    "            runner.load_checkpoint(cfg.load_from)\n",
    "        runner.run(data_loaders, cfg.workflow)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.cfg = Configuration(self.args.cfg_file).define_configuration(self.args)\n",
    "\n",
    "        # Build the dataset\n",
    "        datasets = [build_dataset(self.cfg.data.train)]\n",
    "\n",
    "        # Build the detector\n",
    "        self.model = build_segmentor(\n",
    "            self.cfg.model, train_cfg=self.cfg.get('train_cfg'), test_cfg=self.cfg.get('test_cfg'))\n",
    "        # Add an attribute for visualization convenience\n",
    "        self.model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "        # Create work_dir\n",
    "        mmcv.mkdir_or_exist(os.path.abspath(self.cfg.work_dir))\n",
    "        self.train_segmentor(self.model, datasets, self.cfg, distributed=False, validate=False, meta=dict())\n",
    "\n",
    "        with open(os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.split), 'r') as f:\n",
    "            test_image_list = f.read().splitlines()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        image_dir = os.path.join(os.getcwd(), self.args.work_dir)\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        total_area_intersect = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_union = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_pred_label = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_label = torch.zeros((2,), dtype=torch.float64)\n",
    "\n",
    "        mmcv.mkdir_or_exist(os.path.abspath(self.args.test_dir))\n",
    "        for idx, img_name in enumerate(test_image_list):\n",
    "            # pbar.set_description(f\"Testing {idx+1}/{len(test_image_list)} - {img_name}.jpg\")\n",
    "            print(f\"Testing {idx}/{len(test_image_list)} - {img_name}.jpg\")\n",
    "\n",
    "            img = mmcv.imread(os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.img_dir) + '/' + img_name + '.jpg')\n",
    "            ann = mmcv.imread(os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.ann_dir) + '/' + img_name + '.png')\n",
    "\n",
    "            self.model.cfg = self.cfg\n",
    "\n",
    "            divided_imgs, divided_coor, org_size = divide_img(img, self.model.cfg.crop_size)\n",
    "            result = self.inference_segmentor(self.model, divided_imgs)\n",
    "            result = aggregate_img(result, divided_coor, org_size, self.model.cfg.crop_size)\n",
    "\n",
    "            area_intersect, area_union, area_pred_label, area_label = intersect_and_union(result[0], ann)\n",
    "            total_area_intersect += area_intersect\n",
    "            total_area_union += area_union\n",
    "            total_area_pred_label += area_pred_label\n",
    "            total_area_label += area_label\n",
    "            show_test_result_pyplot(self.model, img, ann, result, title=img_name, image_dir=self.args.test_dir)\n",
    "\n",
    "        precision = total_area_intersect / total_area_pred_label\n",
    "        recall = total_area_intersect / total_area_label\n",
    "        f1 = 2 * ( precision * recall ) / (precision + recall)\n",
    "        iou = total_area_intersect / total_area_union\n",
    "        acc = total_area_intersect / total_area_label\n",
    "        print(f'IoU| Background: {iou[0]}, Bead: {iou[1]}')\n",
    "        print(f'ACC| Background: {acc[0]}, Bead: {acc[1]}')\n",
    "        print(f'Precision| Background: {precision[0]}, Bead: {precision[1]}')\n",
    "        print(f'Recall| Background: {recall[0]}, Bead: {recall[1]}')\n",
    "        print(f'F1| Background: {f1[0]}, Bead: {f1[1]}')\n",
    "\n",
    "    def inference_segmentor(self, model, img):\n",
    "        \"\"\"Inference image with the segmentor.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The loaded segmentor.\n",
    "            img (str/ndarray or list[str/ndarray]): Either image files or loaded\n",
    "                images.\n",
    "\n",
    "        Returns:\n",
    "            (list[Tensor]): The segmentation result.\n",
    "        \"\"\"\n",
    "        cfg = model.cfg\n",
    "        device = next(model.parameters()).device  # model device\n",
    "        # build the data pipeline\n",
    "        test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n",
    "        test_pipeline = Compose(test_pipeline)\n",
    "        # prepare data\n",
    "        if isinstance(img, list):\n",
    "            data = []\n",
    "            for img_patch in img:\n",
    "                data.append(test_pipeline(dict(img=img_patch)))\n",
    "            data = collate(data, samples_per_gpu=1)\n",
    "        else:\n",
    "            data = dict(img=img)\n",
    "            data = test_pipeline(data)\n",
    "            data = collate([data], samples_per_gpu=1)\n",
    "\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            # scatter to specified GPU\n",
    "            data = scatter(data, [device])[0]\n",
    "        else:\n",
    "            data['img_metas'] = [i.data[0] for i in data['img_metas']]\n",
    "        # forward the model\n",
    "        with torch.no_grad():\n",
    "            result = model(return_loss=False, rescale=True, **data)\n",
    "        return result\n",
    "\n",
    "    def inference(self, img_file):\n",
    "        img = mmcv.imread(img_file)\n",
    "        self.cfg = Configuration(self.args.cfg_file).define_configuration(self.args)\n",
    "        self.cfg.model.pretrained = None\n",
    "        self.cfg.model.train_cfg = None\n",
    "        self.model = build_segmentor(self.cfg.model, test_cfg=self.cfg.get('test_cfg'))\n",
    "        checkpoint_file_path = os.path.join(self.args.work_dir, 'latest.pth')\n",
    "\n",
    "        if checkpoint_file_path is not None:\n",
    "            chkpt = load_checkpoint(self.model, checkpoint_file_path, map_location='cpu')\n",
    "            self.model.CLASSES = chkpt['meta']['CLASSES']\n",
    "\n",
    "        self.model.cfg = self.cfg\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        divided_imgs, divided_coor, org_size = divide_img(img, self.model.cfg.crop_size)\n",
    "        result = self.inference_segmentor(self.model, divided_imgs)\n",
    "        result = aggregate_img(result, divided_coor, org_size, self.model.cfg.crop_size)\n",
    "\n",
    "        title = os.path.splitext(os.path.basename(img_file))[0] + '_result'\n",
    "        show_result_pyplot(self.model, img, result, title=title)\n",
    "\n",
    "    def test(self, checkpoints='latest.pth'):\n",
    "        self.cfg = Configuration(self.args.cfg_file).define_configuration(self.args)\n",
    "        self.cfg.model.pretrained = None\n",
    "        self.cfg.model.train_cfg = None\n",
    "        self.model = build_segmentor(self.cfg.model, test_cfg=self.cfg.get('test_cfg'))\n",
    "        checkpoint_file_path = os.path.join(self.args.work_dir, checkpoints)\n",
    "        if checkpoint_file_path is not None:\n",
    "            chkpt = load_checkpoint(self.model, checkpoint_file_path, map_location='cpu')\n",
    "            self.model.CLASSES = chkpt['meta']['CLASSES']\n",
    "\n",
    "        self.model.cfg = self.cfg\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with open(os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.split), 'r') as f:\n",
    "            test_image_list = f.read().splitlines()\n",
    "\n",
    "        image_dir = os.path.join(os.getcwd(), self.args.work_dir)\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "        total_area_intersect = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_union = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_pred_label = torch.zeros((2,), dtype=torch.float64)\n",
    "        total_area_label = torch.zeros((2,), dtype=torch.float64)\n",
    "\n",
    "        mmcv.mkdir_or_exist(os.path.abspath(self.args.test_dir))\n",
    "        for idx, img_name in enumerate(test_image_list):\n",
    "            # pbar.set_description(f\"Testing {idx+1}/{len(test_image_list)} - {img_name}.jpg\")\n",
    "            print(f\"Testing {idx}/{len(test_image_list)} - {img_name}.jpg\")\n",
    "\n",
    "            img = mmcv.imread(\n",
    "                os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.img_dir) + '/' + img_name + '.jpg')\n",
    "            ann = mmcv.imread(\n",
    "                os.path.join(self.cfg.data.val.data_root, self.cfg.data.val.ann_dir) + '/' + img_name + '.png')\n",
    "\n",
    "            self.model.cfg = self.cfg\n",
    "\n",
    "            divided_imgs, divided_coor, org_size = divide_img(img, self.model.cfg.crop_size)\n",
    "            result = self.inference_segmentor(self.model, divided_imgs)\n",
    "            result = aggregate_img(result, divided_coor, org_size, self.model.cfg.crop_size)\n",
    "\n",
    "            area_intersect, area_union, area_pred_label, area_label = intersect_and_union(result[0], ann)\n",
    "            total_area_intersect += area_intersect\n",
    "            total_area_union += area_union\n",
    "            total_area_pred_label += area_pred_label\n",
    "            total_area_label += area_label\n",
    "            show_test_result_pyplot(self.model, img, ann, result, title=img_name, image_dir=self.args.test_dir)\n",
    "\n",
    "        precision = total_area_intersect / total_area_pred_label\n",
    "        recall = total_area_intersect / total_area_label\n",
    "        f1 = 2 * ( precision * recall ) / (precision + recall)\n",
    "        iou = total_area_intersect / total_area_union\n",
    "        acc = total_area_intersect / total_area_label\n",
    "        print(f'IoU| Background: {iou[0]}, Bead: {iou[1]}')\n",
    "        print(f'ACC| Background: {acc[0]}, Bead: {acc[1]}')\n",
    "        print(f'Precision| Background: {precision[0]}, Bead: {precision[1]}')\n",
    "        print(f'Recall| Background: {recall[0]}, Bead: {recall[1]}')\n",
    "        print(f'F1| Background: {f1[0]}, Bead: {f1[1]}')\n",
    "        del self.model\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084ab52",
   "metadata": {},
   "source": [
    "### 비드면 탐지 학습 및 테스트\n",
    "인자 설정 함수에서 max_iters=2000 의 값을 조정한 후 아래 코드를 실행하여 학습을 시작한다. 학습이 완료되면 자동적으로 테스트를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909dd102-a86c-40e9-a161-b1aa9a133f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NVIDIA GeForce RTX 3050 device is available\n",
      "Memory Usage:\n",
      " Allocated: 5.6GB\t Cached: 6.4GB\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA {} device is {}\".format(torch.cuda.get_device_name(0), (\"available\" if torch.cuda.is_available() else \"unavailable\")))\n",
    "print(\"Memory Usage:\\n Allocated: {}GB\\t Cached: {}GB\".format(round(torch.cuda.memory_allocated(0)/1024**3, 1), round(torch.cuda.memory_reserved(0)/1024**3, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab6bc49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 16:18:18,715 - mmseg - INFO - Loaded 80 images\n",
      "C:\\Users\\user\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\backbones\\resnet.py:431: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n",
      "C:\\Users\\user\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\decode_heads\\decode_head.py:94: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert seg_logist into a predictionapplying a threshold\n",
      "  warnings.warn('For binary segmentation, we suggest using'\n",
      "C:\\Users\\user\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\losses\\cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n",
      "2023-03-24 16:18:20,471 - mmseg - INFO - Start running, host: user@DESKTOP-P8UPAA0, work_dir: C:\\jeadae\\용접\\data\\work\n",
      "2023-03-24 16:18:20,472 - mmseg - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2023-03-24 16:18:20,472 - mmseg - INFO - workflow: [('train', 1)], max: 2000 iters\n",
      "2023-03-24 16:18:20,472 - mmseg - INFO - Checkpoints will be saved to C:\\jeadae\\용접\\data\\work by HardDiskBackend.\n",
      "2023-03-24 16:23:41,642 - mmseg - INFO - Saving checkpoint at 500 iterations\n",
      "2023-03-24 16:23:43,618 - mmseg - INFO - Iter [500/2000]\tlr: 7.746e-03, eta: 0:16:08, time: 0.646, data_time: 0.139, memory: 3514, decode.loss_ce: 0.1373, decode.acc_seg: 96.2145, aux.loss_ce: 0.0536, aux.acc_seg: 96.1965, loss: 0.1909\n",
      "2023-03-24 16:29:00,465 - mmseg - INFO - Saving checkpoint at 1000 iterations\n",
      "2023-03-24 16:29:02,387 - mmseg - INFO - Iter [1000/2000]\tlr: 5.410e-03, eta: 0:10:41, time: 0.638, data_time: 0.132, memory: 3514, decode.loss_ce: 0.1116, decode.acc_seg: 96.4132, aux.loss_ce: 0.0444, aux.acc_seg: 96.3864, loss: 0.1561\n",
      "2023-03-24 16:34:24,855 - mmseg - INFO - Saving checkpoint at 1500 iterations\n",
      "2023-03-24 16:34:26,821 - mmseg - INFO - Iter [1500/2000]\tlr: 2.948e-03, eta: 0:05:22, time: 0.649, data_time: 0.143, memory: 3514, decode.loss_ce: 0.0926, decode.acc_seg: 96.7641, aux.loss_ce: 0.0377, aux.acc_seg: 96.7271, loss: 0.1303\n",
      "2023-03-24 16:39:43,477 - mmseg - INFO - Saving checkpoint at 2000 iterations\n",
      "2023-03-24 16:39:45,393 - mmseg - INFO - Iter [2000/2000]\tlr: 1.106e-04, eta: 0:00:00, time: 0.637, data_time: 0.132, memory: 3514, decode.loss_ce: 0.0876, decode.acc_seg: 96.7953, aux.loss_ce: 0.0362, aux.acc_seg: 96.7301, loss: 0.1239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0/20 - 46.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\base.py:289: UserWarning: show==False and out_file is not specified, only result image will be returned\n",
      "  warnings.warn('show==False and out_file is not specified, only '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1/20 - 84.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 5.59 GiB already allocated; 0 bytes free; 6.40 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m segmentor \u001b[38;5;241m=\u001b[39m WeldingSegmentor(def_arguments(), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43msegmentor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 139\u001b[0m, in \u001b[0;36mWeldingSegmentor.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\n\u001b[0;32m    138\u001b[0m divided_imgs, divided_coor, org_size \u001b[38;5;241m=\u001b[39m divide_img(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mcrop_size)\n\u001b[1;32m--> 139\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_segmentor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivided_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m result \u001b[38;5;241m=\u001b[39m aggregate_img(result, divided_coor, org_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mcrop_size)\n\u001b[0;32m    142\u001b[0m area_intersect, area_union, area_pred_label, area_label \u001b[38;5;241m=\u001b[39m intersect_and_union(result[\u001b[38;5;241m0\u001b[39m], ann)\n",
      "Cell \u001b[1;32mIn[4], line 194\u001b[0m, in \u001b[0;36mWeldingSegmentor.inference_segmentor\u001b[1;34m(self, model, img)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 194\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmcv\\runner\\fp16_utils.py:116\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@auto_fp16 can only be used to decorate the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    114\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod of those classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfp16_enabled):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m old_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[0;32m    119\u001b[0m args_info \u001b[38;5;241m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\base.py:110\u001b[0m, in \u001b[0;36mBaseSegmentor.forward\u001b[1;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_train(img, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_test(img, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\base.py:92\u001b[0m, in \u001b[0;36mBaseSegmentor.forward_test\u001b[1;34m(self, imgs, img_metas, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(shape \u001b[38;5;241m==\u001b[39m pad_shapes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m pad_shapes)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_augs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimple_test(imgs[\u001b[38;5;241m0\u001b[39m], img_metas[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug_test(imgs, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\encoder_decoder.py:266\u001b[0m, in \u001b[0;36mEncoderDecoder.simple_test\u001b[1;34m(self, img, img_meta, rescale)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_test\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, img_meta, rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simple test with single image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    268\u001b[0m         seg_pred \u001b[38;5;241m=\u001b[39m (seg_logit \u001b[38;5;241m>\u001b[39m\n\u001b[0;32m    269\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head\u001b[38;5;241m.\u001b[39mthreshold)\u001b[38;5;241m.\u001b[39mto(seg_logit)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\encoder_decoder.py:248\u001b[0m, in \u001b[0;36mEncoderDecoder.inference\u001b[1;34m(self, img, img_meta, rescale)\u001b[0m\n\u001b[0;32m    246\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslide_inference(img, img_meta, rescale)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhole_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    250\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(seg_logit)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\encoder_decoder.py:207\u001b[0m, in \u001b[0;36mEncoderDecoder.whole_inference\u001b[1;34m(self, img, img_meta, rescale)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhole_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, img_meta, rescale):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Inference with full image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     seg_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rescale:\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;66;03m# support dynamic shape for onnx\u001b[39;00m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\encoder_decoder.py:74\u001b[0m, in \u001b[0;36mEncoderDecoder.encode_decode\u001b[1;34m(self, img, img_metas)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, img_metas):\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode images with backbone and decode into a semantic segmentation\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    map of the same size as input.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_head_forward_test(x, img_metas)\n\u001b[0;32m     76\u001b[0m     out \u001b[38;5;241m=\u001b[39m resize(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mout,\n\u001b[0;32m     78\u001b[0m         size\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:],\n\u001b[0;32m     79\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     80\u001b[0m         align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign_corners)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\segmentors\\encoder_decoder.py:66\u001b[0m, in \u001b[0;36mEncoderDecoder.extract_feat\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_feat\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract features from images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_neck:\n\u001b[0;32m     68\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\backbones\\resnet.py:671\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_layers):\n\u001b[0;32m    670\u001b[0m     res_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer_name)\n\u001b[1;32m--> 671\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mres_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_indices:\n\u001b[0;32m    673\u001b[0m         outs\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\backbones\\resnet.py:303\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    301\u001b[0m     out \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mcheckpoint(_inner_forward, x)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_inner_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\mmseg\\models\\backbones\\resnet.py:294\u001b[0m, in \u001b[0;36mBottleneck.forward.<locals>._inner_forward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    291\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_plugin(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_conv3_plugin_names)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:135\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\JDTCH\\lib\\site-packages\\torch\\nn\\functional.py:2146\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2144\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 5.59 GiB already allocated; 0 bytes free; 6.40 GiB reserved in total by PyTorch)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segmentor = WeldingSegmentor(def_arguments(), device='cuda:0')\n",
    "segmentor.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0a158",
   "metadata": {},
   "source": [
    "### 비드면 탐지 테스트\n",
    "미리 학습된 모델을 이용해서, 비드면 탐지 테스트를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be295460",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentor = WeldingSegmentor(def_arguments(), device='cuda:0')\n",
    "segmentor.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e01b1",
   "metadata": {},
   "source": [
    "# 3. 비드면 탐지 결과 이미지 보기\n",
    "비드면 탐지 테스트 실행하면, 총 20건의 용접이미지에 대한 비드면 탐지 결과파일이 test_image 에 저장된다.\n",
    "아래 코드를 실행하여 탐지 결과 이미지를 출력하여 확인할 수 있다.\n",
    "출력 이미지 개수(num_image)를 조정하여 원하는 만큼의 결과를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "import os\n",
    "\n",
    "def display_images(path='test_image', num_images=10):\n",
    "    images = []\n",
    "    names = []\n",
    "    for img_path in sorted(glob.glob(path + '/*.png'), key=lambda name: ''.join(re.split('/|\\.|_', name)[2:4])):\n",
    "        images.append(mpimg.imread(img_path))\n",
    "        names.append(os.path.basename(img_path))\n",
    "    \n",
    "    show_images = images[0:num_images*2]\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    \n",
    "    columns = 4\n",
    "    for i, img in enumerate(show_images):\n",
    "        ax = plt.subplot(int(len(show_images) / columns) + 1, columns, i + 1)\n",
    "        ax.tick_params(left=False, right=False, labelleft = False, labelbottom=False, bottom=False)\n",
    "        plt.imshow(img)\n",
    "        plt.title(names[i])\n",
    "\n",
    "display_images(num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebc09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RootLab",
   "language": "python",
   "name": "jdtch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
